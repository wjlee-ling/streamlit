from modules.preprocessors import BasePreprocessor
from modules.templates_notion import TEMPLATE_NOTION_DEFAULT

import re
import glob
from typing import List, Tuple, Dict, Union, Callable, Optional
from langchain.schema import (
    BasePromptTemplate,
    Document,
    BaseDocumentTransformer,
)
from langchain.document_loaders import NotionDirectoryLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter, MarkdownTextSplitter


class NotionPreprocessor(BasePreprocessor):
    @property
    def splitter(self):
        if self._splitter is None:
            return MarkdownHeaderTextSplitter(headers_to_split_on=[("#", "#")])
        else:
            return self._splitter

    def _get_file_path(self, filename: str, directory: str = None):
        directory = "data/notion/DV-PromptTown"  # ğŸ’¥ TODO: guess from the full path
        files = glob.glob(f"{directory}/**/*{filename}", recursive=True)
        return files

    def _extract_doc_id(self, doc: Document):
        """Notionì´ ìë™ìœ¼ë¡œ ë¶™ì¸ íŒŒì¼/í´ë” id (ì†Œë¬¸ì + ìˆ«ì + (.í™•ì¥ì)) ì¶”ì¶œ."""
        return doc.metadata["source"].split(" ")[-1]

    def _handle_links(
        self,
        doc: Document,
        file_formats: Tuple[str] = ("md", "csv"),
    ) -> Document:
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì—ì„œ í¬í•¨ëœ í•˜ì´í¼ë§í¬ ìŠ¤íŠ¸ë§ì´ ì„ë² ë”© ë˜ì§€ ì•Šê²Œ "(link@{num})"ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸(ì¸ë±ìŠ¤{num})ì— ì €ì¥ (keyëŠ” 'links')"""
        page_content = doc.page_content
        doc.metadata["links"] = []
        while match := re.search(
            r"(?<=\])\(%[A-Za-z0-9\/\(\)%\.~]+",
            page_content,
        ):
            (match_start_idx, non_match_start_idx) = match.span()
            if match.group().strip(")]}").endswith(file_formats):
                # ë§í¬ ìŠ¤íŠ¸ë§ ë©”íƒ€ ë°ì´í„°ì— ì¶”ê°€
                doc.metadata["links"].append(match.group().strip("()"))

                # ë§í¬ ìŠ¤íŠ¸ë§ ì‚­ì œ
                page_content = (
                    page_content[:match_start_idx]
                    + f"(link@{len(doc.metadata['links'])-1})"
                    + page_content[non_match_start_idx:]
                )

            else:
                ## .png ë“±ì€ ê·¸ëƒ¥ ë§í¬ ì‚­ì œ
                page_content = (
                    page_content[:match_start_idx] + page_content[non_match_start_idx:]
                )
        doc.page_content = page_content

        return doc

    def _split(
        self,
        doc: Document,
    ) -> List[Document]:
        """MarkdownHeaderTextSplitterëŠ” `str`ì˜ doc í•˜ë‚˜ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë¯€ë¡œ ì²˜ë¦¬ í›„ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        metadata = doc.metadata
        chunks = self.splitter.split_text(doc.page_content)
        for chunk in chunks:
            chunk.metadata = {**metadata}
        return chunks

    def preprocess_and_split(
        self,
        docs: List[Document],
        fn: Optional[Callable] = None,
    ) -> List[Document]:
        """
        ë³¸ë¬¸ì— í¬í•¨ëœ ë§í¬ë¥¼ placeholderì™€ ë°”ê¾¸ê³ , ë©”íƒ€ë°ì´í„°ë¡œ ì˜®ê¹€
        """
        new_chunks = []
        for doc in docs:
            # ë³¸ë¬¸ì— í¬í•¨ëœ ë§í¬ë¥¼ placeholderì™€ ë°”ê¾¸ê³ , ë©”íƒ€ë°ì´í„°ë¡œ ì˜®ê¹€
            doc = self._handle_links(doc)
            chunks = self._split(doc)
            new_chunks.extend(chunks)
        new_chunks = self._aggregate_chunks(new_chunks)

        return new_chunks

    def _aggregate_chunks(
        self,
        chunks: List[Document],
    ) -> List[Document]:
        """
        `TextSplitter`ê°€ ìë¥¸ ë¬¸ì„œê°€ 1. ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ê³  2. ê°™ì€ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê°–ì„ ë•Œ í•©ì¹˜ê¸° (+ ë©”íƒ€ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì •)
        """
        if len(chunks) == 1:
            return chunks

        prev_chunk = None
        new_chunks = []
        for chunk in chunks:
            if prev_chunk is None:
                # ë§¨ ì²˜ìŒ chunkëŠ” ë°”ë¡œ prev_chunk
                prev_chunk = chunk
                continue

            if (
                len(prev_chunk.page_content) + len(chunk.page_content) < 500
                and prev_chunk.metadata["source"].split("/")[:-1]
                == chunk.metadata["source"].split("/")[:-1]
            ):
                chunk.page_content = prev_chunk.page_content + "\n" + chunk.page_content
                ## ë™ì¼ ë¶€ëª¨ í´ë”ì˜ íŒŒì¼ë“¤ì„ í•©ì¹˜ëŠ” ê²½ìš° "ë¶€ëª¨í´ë”/íŒŒì¼1&&íŒŒì¼2" ë¡œ ë©”íƒ€ ë°ì´í„° ì €ì¥
                chunk.metadata["source"] = (
                    prev_chunk.metadata["source"]
                    + "&&"
                    + chunk.metadata["source"].split("/")[-1]
                )
            else:
                new_chunks.append(prev_chunk)
                self.save_output(
                    {"page_content": chunk.page_content, "metadata": chunk.metadata}
                )

            prev_chunk = chunk

        if prev_chunk != new_chunks[-1]:
            new_chunks.append(prev_chunk)

        return new_chunks


loader = NotionDirectoryLoader("data/test/notion")
docs = loader.load()
processor = NotionPreprocessor()
docs = processor.preprocess_and_split(docs)
print(docs)
