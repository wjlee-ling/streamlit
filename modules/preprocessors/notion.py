from modules.preprocessors import BasePreprocessor
from modules.templates_notion import TEMPLATE_NOTION_DEFAULT

import re
import glob
from typing import List, Tuple, Dict, Union, Callable, Optional
from langchain.schema import (
    BasePromptTemplate,
    Document,
    BaseDocumentTransformer,
)
from langchain.document_loaders import NotionDirectoryLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter, MarkdownTextSplitter


class NotionPreprocessor(BasePreprocessor):
    @property
    def splitter(self):
        if self._splitter is None:
            return MarkdownTextSplitter()
        else:
            return self._splitter

    def _get_file_path(self, filename: str, directory: str = None):
        directory = "data/notion/DV-PromptTown"  # ğŸ’¥ TODO: guess from the full path
        files = glob.glob(f"{directory}/**/*{filename}", recursive=True)
        return files

    def _extract_doc_id(self, doc: Document):
        """Notionì´ ìë™ìœ¼ë¡œ ë¶™ì¸ íŒŒì¼/í´ë” id (ì†Œë¬¸ì + ìˆ«ì + (.í™•ì¥ì)) ì¶”ì¶œ."""
        return doc.metadata["source"].split(" ")[-1]

    def _handle_links(
        self,
        doc: Document,
        file_formats: Tuple[str] = ("md", "csv"),
    ) -> Document:
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì—ì„œ í¬í•¨ëœ í•˜ì´í¼ë§í¬ ìŠ¤íŠ¸ë§ì´ ì„ë² ë”© ë˜ì§€ ì•Šê²Œ "(link@{num})"ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸(ì¸ë±ìŠ¤{num})ì— ì €ì¥ (keyëŠ” 'links')"""
        page_content = doc.page_content
        doc.metadata["links"] = []
        while match := re.search(
            r"(?<=\])\(%[A-Za-z0-9\/\(\)%\.]+",
            page_content,
        ):
            (match_start_idx, non_match_start_idx) = match.span()

            if match.group().endswith(file_formats):
                # ë§í¬ ìŠ¤íŠ¸ë§ ë©”íƒ€ ë°ì´í„°ì— ì¶”ê°€
                doc.metadata["links"].append(match.group().strip("()"))

                # ë§í¬ ìŠ¤íŠ¸ë§ ì‚­ì œ
                page_content = (
                    page_content[: match_start_idx - 1]
                    + f"(link@{len(doc.metadata['links'])})"
                    + page_content[non_match_start_idx:]
                )

            else:
                ## .png ë“±ì€ ê·¸ëƒ¥ ë§í¬ ì‚­ì œ
                page_content = (
                    page_content[: match_start_idx - 1]
                    + page_content[non_match_start_idx:]
                )

        doc.page_content = page_content

        return doc

    def preprocess(
        self,
        docs: List[Document],
        fn: Optional[Callable] = None,
    ) -> List[Document]:
        """
        ë³¸ë¬¸ì— í¬í•¨ëœ ë§í¬ë¥¼ placeholderì™€ ë°”ê¾¸ê³ , ë©”íƒ€ë°ì´í„°ë¡œ ì˜®ê¹€
        """
        fn = fn or self._handle_links
        ## page_contentë‚´ ë§í¬ë¥¼ meta ë°ì´í„°ë¡œ ì¶”ê°€
        docs = [fn(doc) for doc in docs]
        return docs

    def _split(
        self,
        docs: List[Document],
    ) -> List[Document]:
        """`.split_text(doc.page_content)` í•œ ê²°ê³¼ë¬¼ì— ë©”íƒ€ë°ì´í„°ë¡œ í—¤ë”ê°’ ë³¸ë¬¸ì— ì¶”ê°€"""

        chunks = self.splitter.split_documents(docs)  ## ğŸ’¥ header 1ì€ ë„£ëŠ” ê²ƒì´ ë‚˜ì€ì§€?
        chunks = self._aggregate_chunks(chunks)
        return chunks

    def _aggregate_chunks(
        self,
        chunks: List[Document],
    ) -> List[Document]:
        """
        `TextSplitter`ê°€ ìë¥¸ ë¬¸ì„œê°€ 1. ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ê³  2. ê°™ì€ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê°–ì„ ë•Œ í•©ì¹˜ê¸° (+ ë©”íƒ€ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì •)
        """
        prev_chunk = None
        new_chunks = []
        for chunk in chunks:
            if prev_chunk is None:
                # ë§¨ ì²˜ìŒ chunkëŠ” ë°”ë¡œ prev_chjunk
                prev_chunk = chunk
                continue

            if (
                len(prev_chunk.page_content) + len(chunk.page_content) < 500
                and prev_chunk.metadata["source"].split("/")[:-1]
                == chunk.metadata["source"].split("/")[:-1]
            ):
                chunk.page_content = prev_chunk.page_content + "\n" + chunk.page_content
                ## ë™ì¼ ë¶€ëª¨ í´ë”ì˜ íŒŒì¼ë“¤ì„ í•©ì¹˜ëŠ” ê²½ìš° "ë¶€ëª¨í´ë”/íŒŒì¼1&&íŒŒì¼2" ë¡œ ë©”íƒ€ ë°ì´í„° ì €ì¥
                chunk.metadata["source"] = (
                    prev_chunk.metadata["source"]
                    + "&&"
                    + chunk.metadata["source"].split("/")[-1]
                )
            else:
                new_chunks.append(prev_chunk)
                self.save_output(
                    {"page_content": chunk.page_content, "metadata": chunk.metadata}
                )

            prev_chunk = chunk

        if prev_chunk != new_chunks[-1]:
            new_chunks.append(prev_chunk)

        return new_chunks


loader = NotionDirectoryLoader("data/notion/[DV-á„‘á…³á„…á…©á†·á„‘á…³á„á…³á„á…¡á„‹á…®á†«]")
docs = loader.load()
processor = NotionPreprocessor()
docs = processor.preprocess_and_split(docs)
