from modules.preprocessors import BasePreprocessor
from modules.templates_notion import TEMPLATE_NOTION_DEFAULT

import re
import glob
from typing import List, Tuple, Dict, Union, Callable, Optional
from pathlib import Path
from langchain.schema import (
    BasePromptTemplate,
    Document,
    BaseDocumentTransformer,
)
from langchain.document_loaders.base import BaseLoader
from langchain.document_loaders import NotionDirectoryLoader, CSVLoader
from langchain.text_splitter import (
    MarkdownHeaderTextSplitter,
    MarkdownTextSplitter,
    RecursiveCharacterTextSplitter,
)


class NotionPreprocessor(BasePreprocessor):
    def __init__(
        self,
        splitter: Optional[BaseDocumentTransformer] = None,
        sub_splitter: Optional[BaseDocumentTransformer] = None,
        chunk_size: int = 800,
        chunk_overlap: int = 100,
    ):
        super().__init__(splitter=splitter)
        self._splitter = splitter
        self.chunk_size = chunk_size
        self.sub_splitter = sub_splitter or RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )

    @property
    def splitter(self):
        if self._splitter is None:
            return MarkdownHeaderTextSplitter(headers_to_split_on=[("#", "#")])
        else:
            return self._splitter

    def _get_file_path(self, filename: str, directory: str = None):
        directory = "data/notion/DV-PromptTown"  # ğŸ’¥ TODO: guess from the full path
        files = glob.glob(f"{directory}/**/*{filename}", recursive=True)
        return files

    def _extract_doc_id(self, doc: Document):
        """Notionì´ ìë™ìœ¼ë¡œ ë¶™ì¸ íŒŒì¼/í´ë” id (ì†Œë¬¸ì + ìˆ«ì + (.í™•ì¥ì)) ì¶”ì¶œ."""
        return doc.metadata["source"].split(" ")[-1]

    def _handle_links(
        self,
        doc: Document,
        file_formats: Tuple[str] = ("md", "csv"),
    ) -> Document:
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì—ì„œ í¬í•¨ëœ í•˜ì´í¼ë§í¬ ìŠ¤íŠ¸ë§ì´ ì„ë² ë”© ë˜ì§€ ì•Šê²Œ "(link@{num})"ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸(ì¸ë±ìŠ¤{num})ì— ì €ì¥ (keyëŠ” 'links')"""
        page_content = doc.page_content
        page_content_to_process = doc.page_content
        doc.metadata["links"] = []
        while match := re.search(
            r"(?<=\])\(%[A-Za-z0-9\/\(\)%\.~]+",
            page_content_to_process,
        ):
            (match_start_idx, non_match_start_idx) = match.span()
            page_content_to_process = page_content[non_match_start_idx:]
            if match.group().strip(")]}").endswith(file_formats):
                # ë§í¬ ìŠ¤íŠ¸ë§ ë©”íƒ€ ë°ì´í„°ì— ì¶”ê°€
                doc.metadata["links"].append(match.group().strip("()"))

                # ë§í¬ ìŠ¤íŠ¸ë§ ì‚­ì œ
                page_content = (
                    page_content[:match_start_idx]
                    + f"(link@{len(doc.metadata['links'])-1})"
                    + page_content[non_match_start_idx:]
                )

            else:
                ## .png ë“±ì€ ê·¸ëƒ¥ ë§í¬ ì‚­ì œ
                page_content = (
                    page_content[:match_start_idx] + page_content[non_match_start_idx:]
                )
        doc.page_content = page_content

        return doc

    def _split_by_len(self, chunk: str) -> List[str]:
        """self.chunk_size ë³´ë‹¤ ê¸¸ë©´ sub_splitterë¡œ split"""
        if len(chunk) > self.chunk_size:
            return self.sub_splitter.split_text(chunk)
        else:
            return [chunk]

    def _split(
        self,
        doc: Document,
    ) -> List[Document]:
        """
        1. `MarkdownHeaderTextSplitter`ëŠ” `str`ì˜ doc í•˜ë‚˜ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë¯€ë¡œ ì²˜ë¦¬ í›„ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        2. ë©”íƒ€ë°ì´í„°ë¡œ ë“¤ì–´ê°„ í—¤ë” ì •ë³´ ì¶”ê°€
        3. `MarkdownHeaderTextSplitter`ë¡œ ìë¥¸ ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ ë•Œ (ì› ë¬¸ì„œì— í—¤ë”ê°€ ì—†ì–´ì„œ) self.chunk_size ë§Œí¼ ì¶”ê°€ë¡œ ìë¦„
        """
        original_metadata = doc.metadata
        chunks = self.splitter.split_text(doc.page_content)  # split by headers
        new_chunks = []
        for chunk in chunks:
            for header_level, header_content in chunk.metadata.items():
                chunk.page_content = (
                    f"{header_level} {header_content}\n{chunk.page_content}"
                )
            splits_within_max_len = self._split_by_len(chunk.page_content)
            new_chunks.extend(
                [
                    Document(
                        page_content=split, metadata={**original_metadata}
                    )  ## "source" ì™€ "links" ìœ ì§€
                    for split in splits_within_max_len
                ]
            )
        return new_chunks

    def preprocess_and_split(
        self,
        docs: List[Document],
        fn: Optional[Callable] = None,
    ) -> List[Document]:
        new_chunks = []
        for doc in docs:
            # ë³¸ë¬¸ì— í¬í•¨ëœ ë§í¬ë¥¼ placeholderì™€ ë°”ê¾¸ê³ , ë©”íƒ€ë°ì´í„°ë¡œ ì˜®ê¹€
            doc = self._handle_links(doc)
            chunks = self._split(doc)
            new_chunks.extend(chunks)
        new_chunks = self._aggregate_chunks(new_chunks)
        self.save_output(new_chunks)

        return new_chunks

    def _aggregate_chunks(
        self,
        chunks: List[Document],
    ) -> List[Document]:
        """
        `TextSplitter`ê°€ ìë¥¸ ë¬¸ì„œê°€ 1. ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ê³  2. ê°™ì€ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê°–ì„ ë•Œ í•©ì¹˜ê¸° (+ ë©”íƒ€ë°ì´í„° ì†ŒìŠ¤ ìˆ˜ì •)
        """
        if len(chunks) == 1:
            return chunks

        prev_chunk = None
        new_chunks = []
        for chunk in chunks:
            if prev_chunk is None:
                # ë§¨ ì²˜ìŒ chunkëŠ” ë°”ë¡œ prev_chunk
                prev_chunk = chunk
                continue
            if (
                len(prev_chunk.page_content) + len(chunk.page_content) < 500
                and prev_chunk.metadata["source"].split("/")[:-1]
                == chunk.metadata["source"].split("/")[:-1]
            ):
                chunk.page_content = prev_chunk.page_content + "\n" + chunk.page_content
                ## ë™ì¼ ë¶€ëª¨ í´ë”ì˜ íŒŒì¼ë“¤ì„ í•©ì¹˜ëŠ” ê²½ìš° "ë¶€ëª¨í´ë”/íŒŒì¼1&&íŒŒì¼2" ë¡œ ë©”íƒ€ ë°ì´í„° ì €ì¥
                chunk.metadata["source"] = (
                    prev_chunk.metadata["source"]
                    + "&&"
                    + chunk.metadata["source"].split("/")[-1]
                )
            else:
                new_chunks.append(prev_chunk)

            prev_chunk = chunk

        if prev_chunk != new_chunks[-1]:
            new_chunks.append(prev_chunk)

        return new_chunks


class NotionDataLoader(BaseLoader):

    """
    Notion ë°ì´í„° í´ë”ì˜ markdown íŒŒì¼ê³¼ csv íŒŒì¼ë“¤ì„ ë¡œë“œ. í˜„ì¬ `NotionDirectoryLoader`ëŠ” ë””ë ‰í† ë¦¬ì˜ `md` íŒŒì¼ë§Œ
    `CSVLoader`ëŠ” ê¸°íƒ€ csv íŒŒì¼ì„ ì½ìŒ
    ì¼ë‹¨ í˜„ì¬(11/1)ëŠ” '_all'ì˜ ì ‘ë¯¸ì‚¬ê°€ ë¶™ì§€ ì•Šì€ í•˜ìœ„ í˜ì´ì§€ í¬í•¨í•˜ì§€ ì•Šì€ ë°ì´í„°ë² ì´ìŠ¤ë§Œ ì½ìŒ
    """

    def __init__(self, path: str, *, encoding: str = "utf-8-sig") -> None:
        self.encoding = encoding
        self.path = path
        self.MD_Loader = None
        self.CSV_Loader = None

    def _load_markdown(self) -> List[Document]:
        self.MD_Loader = NotionDirectoryLoader(path=self.path, encoding=self.encoding)
        return self.MD_Loader.load()

    def _load_csv(self) -> List[Document]:
        """
        Load csv files that do not have a corresponding `_all.csv` file, meaning they don't contain embedded sub-pages
        """
        csv_files = list(Path(self.path).rglob("*.csv"))
        csv_files = [
            file
            for file in csv_files
            if not file.with_stem(f"{file.stem}_all").exists()
        ]
        docs = []
        for csv_file in csv_files:
            self.CSV_Loader = CSVLoader(file_path=csv_file, encoding=self.encoding)
            docs.extend(self.CSV_Loader.load())
        return docs

    def load(self):
        markdown_docs = self._load_markdown()
        csv_docs = self._load_csv()
        return markdown_docs + csv_docs
